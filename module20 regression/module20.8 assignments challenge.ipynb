{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge\n",
    "In this module, we learned how to approach and solve regression problems using linear regression models. Throughout the module, you worked on a house price dataset from Kaggle. In this challenge, you will keep working on this dataset.\n",
    "\n",
    "The scenario\n",
    "The housing market is one of the most crucial parts of the economy for every country. Purchasing a home is one of the primary ways to build wealth and savings for people. In this respect, predicting prices in the housing market is a very central topic in economic and financial circles.\n",
    "\n",
    "The house price dataset from Kaggle includes several features of the houses along with their sale prices at the time they are sold. So far, in this module, you built and implemented some models using this dataset.\n",
    "\n",
    "In this challenge, you are required to improve your model with respect to its prediction performance.\n",
    "\n",
    "To complete this challenge, submit a Jupyter notebook containing your solutions to the following tasks.\n",
    "\n",
    "Steps\n",
    "1. Load the houseprices data from Thinkful's database.\n",
    "\n",
    "2. Do data cleaning, exploratory data analysis, and feature engineering. You can use your previous work in this module. But make sure that your work is satisfactory.\n",
    "\n",
    "3. Now, split your data into train and test sets where 20% of the data resides in the test set.\n",
    "\n",
    "4. Build several linear regression models including Lasso, Ridge, or ElasticNet and train them in the training set. Use k-fold cross-validation to select the best hyperparameters if your models include one!\n",
    "\n",
    "5. Evaluate your best model on the test set.\n",
    "\n",
    "6. So far, you have only used the features in the dataset. However, house prices can be affected by many factors like economic activity and the interest rates at the time they are sold. So, try to find some useful factors that are not included in the dataset. Integrate these factors into your model and assess the prediction performance of your model. Discuss the implications of adding these external variables into your model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load the houseprices data from Thinkful's database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.tools.eval_measures import mse, rmse\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "import math\n",
    "import seaborn as sns\n",
    "\n",
    "import scipy.stats as stats\n",
    "\n",
    "# Display preferences.\n",
    "%matplotlib inline\n",
    "pd.options.display.float_format = '{:.3f}'.format\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action=\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [],
   "source": [
    "postgres_user = 'dsbc_student'\n",
    "postgres_pw = '7*.8G9QH21'\n",
    "postgres_host = '142.93.121.174'\n",
    "postgres_port = '5432'\n",
    "postgres_db = 'houseprices'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = create_engine('postgresql://{}:{}@{}:{}/{}'.format(\n",
    "    postgres_user, postgres_pw, postgres_host, postgres_port, postgres_db))\n",
    "df = pd.read_sql_query('select * from houseprices',con=engine)\n",
    "\n",
    "# no need for an open connection, as we're only doing a single query\n",
    "engine.dispose()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Do data cleaning, exploratory data analysis, and feature engineering. You can use your previous work in this module. But make sure that your work is satisfactory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get rid of the columns with nan. (since we don't need that many features)\n",
    "df_drop = df.copy()\n",
    "for col in df_drop.columns:\n",
    "    if df_drop[col].isna().any():\n",
    "#         print(col)\n",
    "        df_drop = df_drop.drop(columns=col)    \n",
    "\n",
    "# in this regression task, let's use continous type features\n",
    "for (col,col_type) in zip(df_drop.columns,df_drop.dtypes):\n",
    "    if col_type == 'object':\n",
    "        df_drop = df_drop.drop(columns=col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check correlation using heatmap\n",
    "corr_matrix = np.abs(df_drop.corr())\n",
    "plt.figure(figsize=(20,20))\n",
    "sns.heatmap(corr_matrix, square=True, annot=True, linewidths=.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### when the correlation between feature and target varialbe is too low (< 0.1), those features might not be helpful to explain the target variable in our linear regression model, so we can drop them.\n",
    "also, we would like to avoid colinearity between features, otherwise might cause unstability, so we can drop features in the high correlation coefficient pairs.\n",
    "\n",
    "# algorithm:\n",
    "\n",
    "- 1. check features pair with collinearity higher than certain threshold (i.e. > 0.5)\n",
    "- 2. in the feature pair, get rid of the one has lower correalation coefficient(cc) on target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_drop_2 = df_drop.copy()\n",
    "\n",
    "corr_matrix = np.abs(df_drop_2.corr())\n",
    "col_index_list = []\n",
    "for (index, val) in enumerate(corr_matrix.iloc[:,-1]):\n",
    "#     print (index, val)\n",
    "    if val > 0.3 :\n",
    "        col_index_list.append(index)\n",
    "col_index_list\n",
    "\n",
    "df_drop_2 = df_drop_2.iloc[:, col_index_list].copy()\n",
    "\n",
    "corr_matrix = np.abs(df_drop_2.corr())\n",
    "plt.figure(figsize=(10,10))\n",
    "sns.heatmap(corr_matrix, square=True, annot=True, linewidths=.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort columns based on the correalation coefficient between feature and target,\n",
    "# so that the algorithm will be easier to apply\n",
    "df_drop_3 = df_drop_2.copy()\n",
    "corr_matrix = np.abs(df_drop_3.corr())\n",
    "s_sort = corr_matrix.iloc[:,-1].sort_values()\n",
    "df_drop_3 = df_drop_3.loc[:,s_sort.index]\n",
    "\n",
    "corr_matrix = np.abs(df_drop_3.corr())\n",
    "\n",
    "corr_matrix = np.abs(df_drop_3.corr())\n",
    "plt.figure(figsize=(10,10))\n",
    "sns.heatmap(corr_matrix, square=True, annot=True, linewidths=.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the columns when collinearity between features is higher than threshold,\n",
    "# since we've already sorted the columns, so the lower cc between feature and target will drop out first\n",
    "\n",
    "df_drop_4 = df_drop_3.copy()\n",
    "corr_thres = 0.5\n",
    "count = 15\n",
    "\n",
    "\n",
    "corr_matrix = np.abs(df_drop_4.corr())\n",
    "corr_matrix.iloc[:-1,:-1][np.abs(corr_matrix)==1] = -10\n",
    "while((corr_matrix.iloc[:-1,:-1]>corr_thres).any(axis=None)   and count >0):\n",
    "    col_current = 0\n",
    "    while col_current < len(corr_matrix):        \n",
    "        if (corr_matrix.iloc[col_current,:-1]>corr_thres).any():\n",
    "#             print(\"current col: \", col_current)\n",
    "            df_drop_4.drop(columns=df_drop_4.columns[col_current], axis=1, inplace=True)\n",
    "            corr_matrix = np.abs(df_drop_4.corr())\n",
    "            corr_matrix.iloc[:-1,:-1][np.abs(corr_matrix)==1] = -10  \n",
    "            col_current = len(corr_matrix)\n",
    "        else:        \n",
    "            col_current +=1\n",
    "#             print(\"current col: \", col_current)\n",
    "                      \n",
    "    count -=1\n",
    "#     print(\"count is: \", count)\n",
    "#     print(df_drop_4.columns)\n",
    "\n",
    "# check the cc again\n",
    "corr_matrix = np.abs(df_drop_4.corr())\n",
    "plt.figure(figsize=(10,10))\n",
    "sns.heatmap(corr_matrix, square=True, annot=True, linewidths=.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### we will pick the remained variables as the features to feed in the linear regression model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick the features\n",
    "# X = df[['openporchsf', 'wooddecksf', 'fireplaces', 'overallqual']]\n",
    "X = df_drop_4\n",
    "X = X.drop(columns=['saleprice'])\n",
    "Y = df['saleprice']\n",
    "# Y = np.log1p(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()\n",
    "# X.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Now, split your data into train and test sets where 20% of the data resides in the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size = 0.2, random_state = 465)\n",
    "\n",
    "print(\"The number of observations in training set is {}\".format(X_train.shape[0]))\n",
    "print(\"The number of observations in test set is {}\".format(X_test.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Build several linear regression models including Lasso, Ridge, or ElasticNet and train them in the training set. Use k-fold cross-validation to select the best hyperparameters if your models include one!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression, LassoCV, RidgeCV, ElasticNetCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start with OLS    \n",
    "\n",
    "def lrm_myfun(X_train, X_test, y_train, y_test): \n",
    "    lrm = LinearRegression()\n",
    "\n",
    "    lrm.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "    # We are making predictions here\n",
    "    y_preds_train = lrm.predict(X_train)\n",
    "    y_preds_test = lrm.predict(X_test)\n",
    "\n",
    "\n",
    "    print(\"-----OLS Training set statistics-----\")\n",
    "    print(\"constant: {}, coefficient: {}\".format(lrm.intercept_, lrm.coef_))\n",
    "    print(\"R-squared of the model in training set is: {}\".format(lrm.score(X_train, y_train)))\n",
    "    print(\"-----OLS Test set statistics-----\")\n",
    "    print(\"R-squared of the model in test set is: {}\".format(lrm.score(X_test, y_test)))\n",
    "    print(\"Mean absolute error of the prediction is: {}\".format(mean_absolute_error(y_test, y_preds_test)))\n",
    "    print(\"Mean squared error of the prediction is: {}\".format(mse(y_test, y_preds_test)))\n",
    "    print(\"Root mean squared error of the prediction is: {}\".format(rmse(y_test, y_preds_test)))\n",
    "    print(\"Mean absolute percentage error of the prediction is: {}\".format(np.mean(np.abs((y_test - y_preds_test) / y_test)) * 100))\n",
    "\n",
    "    # make a scatter plot to compare predicated and actual\n",
    "\n",
    "    plt.scatter(y_test, y_preds_test)\n",
    "    plt.plot(y_test, y_test, color=\"red\")\n",
    "    plt.xlabel(\"true values\")\n",
    "    plt.ylabel(\"predicted values\")\n",
    "    plt.title(\"OLS: true and predicted values\")\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "    \n",
    "lrm_myfun(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# lasso regression\n",
    "\n",
    "# making a list of alphas(lambdas)\n",
    "alphas = [np.power(10.0,p) for p in np.arange(-10,40,1)]\n",
    "\n",
    "# split\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size = 0.2, random_state = 465)\n",
    "\n",
    "def lasso_myfun(X_train, X_test, y_train, y_test, alphas):\n",
    "\n",
    "    lasso_cv = LassoCV(alphas=alphas, cv=5)\n",
    "\n",
    "    lasso_cv.fit(X_train, y_train)\n",
    "    \n",
    "    # We are making predictions here\n",
    "    y_preds_train = lasso_cv.predict(X_train)\n",
    "    y_preds_test = lasso_cv.predict(X_test)\n",
    "\n",
    "    # note: lasso_cv.alpha_ return the best alpha\n",
    "    print(\"-----Lasso Training set statistics-----\")\n",
    "    print(\"constant: {}, coefficient: {}\".format(lasso_cv.intercept_, lasso_cv.coef_))\n",
    "    print(\"Best alpha value is: {}\".format(lasso_cv.alpha_))\n",
    "    print(\"R-squared of the model in training set is: {}\".format(lasso_cv.score(X_train, y_train)))\n",
    "    print(\"-----Lasso Test set statistics-----\")\n",
    "    print(\"R-squared of the model in test set is: {}\".format(lasso_cv.score(X_test, y_test)))\n",
    "    print(\"Mean absolute error of the prediction is: {}\".format(mean_absolute_error(y_test, y_preds_test)))\n",
    "    print(\"Mean squared error of the prediction is: {}\".format(mse(y_test, y_preds_test)))\n",
    "    print(\"Root mean squared error of the prediction is: {}\".format(rmse(y_test, y_preds_test)))\n",
    "    print(\"Mean absolute percentage error of the prediction is: {}\".format(np.mean(np.abs((y_test - y_preds_test) / y_test)) * 100))\n",
    "\n",
    "\n",
    "\n",
    "    # make a scatter plot to compare predicated and actual\n",
    "\n",
    "    plt.scatter(y_test, y_preds_test)\n",
    "    plt.plot(y_test, y_test, color=\"red\")\n",
    "    plt.xlabel(\"true values\")\n",
    "    plt.ylabel(\"predicted values\")\n",
    "    plt.title(\"Lasso: true and predicted values\")\n",
    "    plt.show()\n",
    "    \n",
    "lasso_myfun(X_train, X_test, y_train, y_test, alphas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ridge regreesion + cross validation (CV)\n",
    "\n",
    "\n",
    "def ridge_myfun(X_train, X_test, y_train, y_test, alphas):\n",
    "    ridge_cv = RidgeCV(alphas=alphas, cv=5)\n",
    "\n",
    "    ridge_cv.fit(X_train, y_train)\n",
    "    \n",
    "\n",
    "    # We are making predictions here\n",
    "    y_preds_train = ridge_cv.predict(X_train)\n",
    "    y_preds_test = ridge_cv.predict(X_test)\n",
    "\n",
    "    # note: ridge_cv.alpha_ return the best alpha\n",
    "    print(\"-----Ridge Training set statistics-----\")\n",
    "    print(\"constant: {}, coefficient: {}\".format(ridge_cv.intercept_, ridge_cv.coef_))\n",
    "    print(\"Best alpha value is: {}\".format(ridge_cv.alpha_))\n",
    "    print(\"R-squared of the model in training set is: {}\".format(ridge_cv.score(X_train, y_train)))\n",
    "    print(\"-----Ridge {Test set statistics-----\")\n",
    "    print(\"R-squared of the model in test set is: {}\".format(ridge_cv.score(X_test, y_test)))\n",
    "    print(\"Mean absolute error of the prediction is: {}\".format(mean_absolute_error(y_test, y_preds_test)))\n",
    "    print(\"Mean squared error of the prediction is: {}\".format(mse(y_test, y_preds_test)))\n",
    "    print(\"Root mean squared error of the prediction is: {}\".format(rmse(y_test, y_preds_test)))\n",
    "    print(\"Mean absolute percentage error of the prediction is: {}\".format(np.mean(np.abs((y_test - y_preds_test) / y_test)) * 100))\n",
    "\n",
    "    # make a scatter plot to compare predicated and actual\n",
    "\n",
    "    plt.scatter(y_test, y_preds_test)\n",
    "    plt.plot(y_test, y_test, color=\"red\")\n",
    "    plt.xlabel(\"true values\")\n",
    "    plt.ylabel(\"predicted values\")\n",
    "    plt.title(\"Ridge: true and predicted values\")\n",
    "    plt.show()\n",
    "    \n",
    "ridge_myfun(X_train, X_test, y_train, y_test, alphas)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ElasticNet\n",
    "\n",
    "\n",
    "def elasticnet_myfun(X_train, X_test, y_train, y_test, alphas):\n",
    "    elasticnet_cv = ElasticNetCV(alphas=alphas, cv=5)\n",
    "\n",
    "    elasticnet_cv.fit(X_train, y_train)\n",
    "    \n",
    "\n",
    "    # We are making predictions here\n",
    "    y_preds_train = elasticnet_cv.predict(X_train)\n",
    "    y_preds_test = elasticnet_cv.predict(X_test)\n",
    "\n",
    "    # note: lasso_cv.alpha_ return the best alpha\n",
    "    print(\"-----ElasticNet Training set statistics-----\")\n",
    "    print(\"constant: {}, coefficient: {}\".format(elasticnet_cv.intercept_, elasticnet_cv.coef_))\n",
    "    print(\"Best alpha value is: {}\".format(elasticnet_cv.alpha_))\n",
    "    print(\"R-squared of the model in training set is: {}\".format(elasticnet_cv.score(X_train, y_train)))\n",
    "    print(\"-----ElasticNet Test set statistics-----\")\n",
    "    print(\"R-squared of the model in test set is: {}\".format(elasticnet_cv.score(X_test, y_test)))\n",
    "    print(\"Mean absolute error of the prediction is: {}\".format(mean_absolute_error(y_test, y_preds_test)))\n",
    "    print(\"Mean squared error of the prediction is: {}\".format(mse(y_test, y_preds_test)))\n",
    "    print(\"Root mean squared error of the prediction is: {}\".format(rmse(y_test, y_preds_test)))\n",
    "    print(\"Mean absolute percentage error of the prediction is: {}\".format(np.mean(np.abs((y_test - y_preds_test) / y_test)) * 100))\n",
    "\n",
    "    # make a scatter plot to compare predicated and actual\n",
    "\n",
    "    plt.scatter(y_test, y_preds_test)\n",
    "    plt.plot(y_test, y_test, color=\"red\")\n",
    "    plt.xlabel(\"true values\")\n",
    "    plt.ylabel(\"predicted values\")\n",
    "    plt.title(\"ElasticNet: true and predicted values\")\n",
    "    plt.show()\n",
    "\n",
    "elasticnet_myfun(X_train, X_test, y_train, y_test, alphas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Evaluate your best model on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# sumarize the result\n",
    "\n",
    "# pick the features\n",
    "X = df[['openporchsf', 'wooddecksf', 'fireplaces', 'overallqual']]\n",
    "Y = df['saleprice']\n",
    "\n",
    "# split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size = 0.2, random_state = 465)\n",
    "\n",
    "# making a list of alphas(lambdas)\n",
    "alphas = [np.power(10.0,p) for p in np.arange(-10,40,1)]\n",
    "\n",
    "lrm_myfun(X_train, X_test, y_train, y_test)\n",
    "lasso_myfun(X_train, X_test, y_train, y_test, alphas)\n",
    "ridge_myfun(X_train, X_test, y_train, y_test, alphas)\n",
    "elasticnet_myfun(X_train, X_test, y_train, y_test, alphas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### evaluation sumary\n",
    "\n",
    "- As the results showed, when taken 'openporchsf', 'wooddecksf', 'fireplaces', 'overallqual' as the feautures, \"Sale price\" as the target, Ridge regression is the best performance model (to be more specific when alpha taken 10.0). \n",
    "- Among other stats, Ridge regression model has the lowest MAPE value (21.427533081169933%) on test set, and the highest R-squared score (0.6413897187301367 on test set, 0.6810531255539183 on training set).\n",
    "- The estimation coefficients (including constant term) is: constant: -81984.59555811743, coefficient: [   87.78736864    80.28724956 19299.6517318  39315.75092182]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5b extra comment, iterative step, feature engineering\n",
    "\n",
    "- Before we jump into searching for new features and include them in the model, we can actually take more usage of the features we already have. \n",
    "- For instance, we realize that the target varialbe has kind of skewed distribution. We can try to fix that by performing -  log1p transformation (i.e. log1p = log(1+x)), and hope that can improve the performance.\n",
    "- And it does!!!\n",
    "\n",
    "### notes:\n",
    "- The illustration is performed using ElasticNet model.\n",
    "- Note that, in the evaluation part, we use transformed data to calculate R-squared value, but use the invsered-transformed data (original data) to calculate MAPE (mean absolute percentage error).\n",
    "- Also note that, the new model using transformed target data has two very low coefficient values associated with 'openporchsf' and 'wooddecksf'. When excluding those two features from our model, we only lost less than one percentage of MAPE. This is a good news when a less complex model is favored than one with better predication capability. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### modification (feature engineering)\n",
    "\n",
    "Y = df['saleprice']\n",
    "sns.distplot(Y)\n",
    "\n",
    "print(stats.describe(Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick the features\n",
    "X = df[['openporchsf', 'wooddecksf', 'fireplaces', 'overallqual']]\n",
    "Y = df['saleprice']\n",
    "# log1p transforming Y\n",
    "Y = np.log1p(Y)\n",
    "\n",
    "# split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size = 0.2, random_state = 465)\n",
    "\n",
    "print(\"The number of observations in training set is {}\".format(X_train.shape[0]))\n",
    "print(\"The number of observations in test set is {}\".format(X_test.shape[0]))\n",
    "\n",
    "# ElasticNet\n",
    "elasticnet_cv = ElasticNetCV(alphas=alphas, cv=5)\n",
    "\n",
    "elasticnet_cv.fit(X_train, y_train)\n",
    "print(\"constant: {}, coefficient: {}\".format(elasticnet_cv.intercept_, elasticnet_cv.coef_))\n",
    "\n",
    "# We are making predictions here\n",
    "y_preds_train = elasticnet_cv.predict(X_train)\n",
    "y_preds_test = elasticnet_cv.predict(X_test)\n",
    "\n",
    "print(\"Best alpha value is: {}\".format(elasticnet_cv.alpha_))\n",
    "print(\"R-squared of the model in training set is: {}\".format(elasticnet_cv.score(X_train, y_train)))\n",
    "print(\"-----Test set statistics-----\")\n",
    "print(\"R-squared of the model in test set is: {}\".format(elasticnet_cv.score(X_test, y_test)))\n",
    "\n",
    "y_train_invtrans = np.expm1(y_train)\n",
    "y_test_invtrans = np.expm1(y_test)\n",
    "y_preds_train_invtrans = np.expm1(y_preds_train)\n",
    "y_preds_test_invtrans = np.expm1(y_preds_test)\n",
    "\n",
    "\n",
    "print(\"-----Test set statistics (compare with original-----\")\n",
    "print(\"Mean absolute error of the prediction is: {}\".format(mean_absolute_error(y_test_invtrans, y_preds_test_invtrans)))\n",
    "print(\"Mean squared error of the prediction is: {}\".format(mse(y_test_invtrans, y_preds_test_invtrans)))\n",
    "print(\"Root mean squared error of the prediction is: {}\".format(rmse(y_test_invtrans, y_preds_test_invtrans)))\n",
    "print(\"Mean absolute percentage error of the prediction is: {}\".format(np.mean(np.abs((y_test_invtrans - y_preds_test_invtrans) / y_test_invtrans)) * 100))\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.subplot(1,2,1)\n",
    "plt.scatter(y_test_invtrans, y_preds_test_invtrans)\n",
    "plt.plot(y_test_invtrans, y_test_invtrans, color=\"red\")\n",
    "plt.xlabel(\"true values\")\n",
    "plt.ylabel(\"predicted values\")\n",
    "plt.title(\"ElectNet: true and predicted values (original)\")\n",
    "\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.scatter(y_test, y_preds_test)\n",
    "plt.plot(y_test, y_test, color=\"red\")\n",
    "plt.xlabel(\"true values\")\n",
    "plt.ylabel(\"predicted values\")\n",
    "plt.title(\"ElectNet: true and predicted values (transformed)\")\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trail 2, get only take two features.\n",
    "\n",
    "# pick the features\n",
    "X = df[['fireplaces', 'overallqual']]\n",
    "Y = df['saleprice']\n",
    "Y = np.log1p(Y)\n",
    "\n",
    "# split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size = 0.2, random_state = 465)\n",
    "\n",
    "print(\"The number of observations in training set is {}\".format(X_train.shape[0]))\n",
    "print(\"The number of observations in test set is {}\".format(X_test.shape[0]))\n",
    "\n",
    "# ElasticNet\n",
    "elasticnet_cv = ElasticNetCV(alphas=alphas, cv=5)\n",
    "\n",
    "elasticnet_cv.fit(X_train, y_train)\n",
    "print(\"constant: {}, coefficient: {}\".format(elasticnet_cv.intercept_, elasticnet_cv.coef_))\n",
    "\n",
    "# We are making predictions here\n",
    "y_preds_train = elasticnet_cv.predict(X_train)\n",
    "y_preds_test = elasticnet_cv.predict(X_test)\n",
    "\n",
    "print(\"Best alpha value is: {}\".format(elasticnet_cv.alpha_))\n",
    "print(\"R-squared of the model in training set is: {}\".format(elasticnet_cv.score(X_train, y_train)))\n",
    "print(\"-----Test set statistics-----\")\n",
    "print(\"R-squared of the model in test set is: {}\".format(elasticnet_cv.score(X_test, y_test)))\n",
    "\n",
    "y_train_invtrans = np.expm1(y_train)\n",
    "y_test_invtrans = np.expm1(y_test)\n",
    "y_preds_train_invtrans = np.expm1(y_preds_train)\n",
    "y_preds_test_invtrans = np.expm1(y_preds_test)\n",
    "\n",
    "\n",
    "print(\"-----Test set statistics (compare with original-----\")\n",
    "print(\"Mean absolute error of the prediction is: {}\".format(mean_absolute_error(y_test_invtrans, y_preds_test_invtrans)))\n",
    "print(\"Mean squared error of the prediction is: {}\".format(mse(y_test_invtrans, y_preds_test_invtrans)))\n",
    "print(\"Root mean squared error of the prediction is: {}\".format(rmse(y_test_invtrans, y_preds_test_invtrans)))\n",
    "print(\"Mean absolute percentage error of the prediction is: {}\".format(np.mean(np.abs((y_test_invtrans - y_preds_test_invtrans) / y_test_invtrans)) * 100))\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.subplot(1,2,1)\n",
    "plt.scatter(y_test_invtrans, y_preds_test_invtrans)\n",
    "plt.plot(y_test_invtrans, y_test_invtrans, color=\"red\")\n",
    "plt.xlabel(\"true values\")\n",
    "plt.ylabel(\"predicted values\")\n",
    "plt.title(\"ElectNet: true and predicted values (original)\")\n",
    "\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.scatter(y_test, y_preds_test)\n",
    "plt.plot(y_test, y_test, color=\"red\")\n",
    "plt.xlabel(\"true values\")\n",
    "plt.ylabel(\"predicted values\")\n",
    "plt.title(\"ElectNet: true and predicted values (transformed)\")\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. So far, you have only used the features in the dataset. However, house prices can be affected by many factors like economic activity and the interest rates at the time they are sold. So, try to find some useful factors that are not included in the dataset. Integrate these factors into your model and assess the prediction performance of your model. Discuss the implications of adding these external variables into your model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### potential solution\n",
    "- Interest rates can be an important factor on house sale price. But for individual buys, it is the mortgage interest rate one can get matters, rather than the Federal Reserve Interest Rate. Note that the personal mortgage interest rate can be consdiered as interaction variables related to house type (i.e. townhouse, condominium, single-family, etc.), credit, profile, property use, so on and so forth. \n",
    "- While it is very hard to exact personal mortgage interest rate for each sale, maybe we can focus on some key factors that contribute to personal mortgage interest rate. Here, let's use \"quality\" and \"house size\". Note that: \"quality\" can be indicated by \"OverallQual\" and/or \"OverallCond\" variables, \"house size\" can be calcualted by using \"totalbsmtsf\" + \"firstflrsf\" + \"secondflrsf\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # new features\n",
    "# X['totalsf'] = df['totalbsmtsf'] + df['firstflrsf'] + df['secondflrsf']\n",
    "# X['int_over_sf'] = X['totalsf'] * X['overallqual']\n",
    "\n",
    "# X = pd.DataFrame()\n",
    "X = df[['fireplaces', 'overallqual']]\n",
    "X['int_over_sf'] = (df['totalbsmtsf'] + df['firstflrsf'] + df['secondflrsf']) * df['overallqual']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# pick the features\n",
    "# X = df[['fireplaces', 'overallqual']]\n",
    "X =X\n",
    "Y = df['saleprice']\n",
    "Y = np.log1p(Y)\n",
    "\n",
    "# split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size = 0.2, random_state = 465)\n",
    "\n",
    "print(\"The number of observations in training set is {}\".format(X_train.shape[0]))\n",
    "print(\"The number of observations in test set is {}\".format(X_test.shape[0]))\n",
    "\n",
    "# ElasticNet\n",
    "elasticnet_cv = ElasticNetCV(alphas=alphas, cv=5)\n",
    "\n",
    "elasticnet_cv.fit(X_train, y_train)\n",
    "print(\"constant: {}, coefficient: {}\".format(elasticnet_cv.intercept_, elasticnet_cv.coef_))\n",
    "\n",
    "# We are making predictions here\n",
    "y_preds_train = elasticnet_cv.predict(X_train)\n",
    "y_preds_test = elasticnet_cv.predict(X_test)\n",
    "\n",
    "print(\"Best alpha value is: {}\".format(elasticnet_cv.alpha_))\n",
    "print(\"R-squared of the model in training set is: {}\".format(elasticnet_cv.score(X_train, y_train)))\n",
    "print(\"-----Test set statistics-----\")\n",
    "print(\"R-squared of the model in test set is: {}\".format(elasticnet_cv.score(X_test, y_test)))\n",
    "\n",
    "y_train_invtrans = np.expm1(y_train)\n",
    "y_test_invtrans = np.expm1(y_test)\n",
    "y_preds_train_invtrans = np.expm1(y_preds_train)\n",
    "y_preds_test_invtrans = np.expm1(y_preds_test)\n",
    "\n",
    "\n",
    "print(\"-----Test set statistics (compare with original-----\")\n",
    "print(\"Mean absolute error of the prediction is: {}\".format(mean_absolute_error(y_test_invtrans, y_preds_test_invtrans)))\n",
    "print(\"Mean squared error of the prediction is: {}\".format(mse(y_test_invtrans, y_preds_test_invtrans)))\n",
    "print(\"Root mean squared error of the prediction is: {}\".format(rmse(y_test_invtrans, y_preds_test_invtrans)))\n",
    "print(\"Mean absolute percentage error of the prediction is: {}\".format(np.mean(np.abs((y_test_invtrans - y_preds_test_invtrans) / y_test_invtrans)) * 100))\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.subplot(1,2,1)\n",
    "plt.scatter(y_test_invtrans, y_preds_test_invtrans)\n",
    "plt.plot(y_test_invtrans, y_test_invtrans, color=\"red\")\n",
    "plt.xlabel(\"true values\")\n",
    "plt.ylabel(\"predicted values\")\n",
    "plt.title(\"ElectNet: true and predicted values (original)\")\n",
    "\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.scatter(y_test, y_preds_test)\n",
    "plt.plot(y_test, y_test, color=\"red\")\n",
    "plt.xlabel(\"true values\")\n",
    "plt.ylabel(\"predicted values\")\n",
    "plt.title(\"ElectNet: true and predicted values (transformed)\")\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sumary\n",
    "\n",
    "- After including the new feature \"int_over_sf\", the performance has improved. (i.e. PAME = 15.923998640777748 from 17.968835375052954).\n",
    "- the final model has three features, \"fireplaces\", \"overallqual\", \"int_over_sf\". The former two features are directly from the dataset, while the last one is an interaction varialbe, that we try to model the effect of personal interest rate. \n",
    "- Note that \"int_over_sf\" feature is \"calculated\" from features we have in the dataset. (i.e. X['int_over_sf'] = (df['totalbsmtsf'] + df['firstflrsf'] + df['secondflrsf']) * df['overallqual']). By including such interaction variables, the model performance has improved. Which indicates that, sometimes, we shall try to explore the data we already have instead of feeding \"new\" data into the model right away."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
